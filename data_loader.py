from __future__ import annotations

"""Data loading + sidebar filters.

Performance goal: do not load the full inventory table on page load.
We load only lightweight metadata (regions, location list, date bounds) until the
user submits filters.

Note: we intentionally avoid importing `admin_config` at module import time to
prevent circular imports (admin_config imports data_loader).
"""

import pandas as pd
import streamlit as st

from datetime import date, timedelta

from config import (
    DATA_SOURCE,
    SQLITE_DB_PATH,
    SQLITE_TABLE,
    SQLITE_SOURCE_STATUS_TABLE,
    SNOWFLAKE_WAREHOUSE,
    SNOWFLAKE_SOURCE_STATUS_TABLE,
    RAW_INVENTORY_TABLE,
    COL_ADJUSTMENTS,
    COL_AVAILABLE_SPACE,
    COL_BATCH_IN_RAW,
    COL_BATCH_OUT_RAW,
    COL_CLOSE_INV_RAW,
    COL_GAIN_LOSS,
    COL_OPEN_INV_RAW,
    COL_PIPELINE_IN,
    COL_PIPELINE_OUT,
    COL_PRODUCTION,
    COL_RACK_LIFTINGS_RAW,
    COL_SAFE_FILL_LIMIT,
    COL_TANK_CAPACITY,
    COL_TRANSFERS,
)

NUMERIC_COLUMN_MAP = {
    COL_BATCH_IN_RAW: "RECEIPTS_BBL",
    COL_BATCH_OUT_RAW: "DELIVERIES_BBL",
    COL_RACK_LIFTINGS_RAW: "RACK_LIFTINGS_BBL",
    COL_CLOSE_INV_RAW: "CLOSING_INVENTORY_BBL",
    COL_OPEN_INV_RAW: "OPENING_INVENTORY_BBL",
    COL_PRODUCTION: "PRODUCTION_BBL",
    COL_PIPELINE_IN: "PIPELINE_IN_BBL",
    COL_PIPELINE_OUT: "PIPELINE_OUT_BBL",
    COL_ADJUSTMENTS: "ADJUSTMENTS_BBL",
    COL_GAIN_LOSS: "GAIN_LOSS_BBL",
    COL_TRANSFERS: "TRANSFERS_BBL",
    COL_TANK_CAPACITY: "TANK_CAPACITY_BBL",
    COL_SAFE_FILL_LIMIT: "SAFE_FILL_LIMIT_BBL",
    COL_AVAILABLE_SPACE: "AVAILABLE_SPACE_BBL",
}


def _col(raw_df: pd.DataFrame, name: str, default=None) -> pd.Series:
    if name in raw_df.columns:
        return raw_df[name]
    return pd.Series([default] * len(raw_df), index=raw_df.index)


@st.cache_resource(show_spinner=False)
def get_snowflake_session():
    from snowflake.snowpark.context import get_active_session  # type: ignore

    return get_active_session()


def _normalize_inventory_df(raw_df: pd.DataFrame) -> pd.DataFrame:
    df = pd.DataFrame(index=raw_df.index)

    df["Date"] = pd.to_datetime(_col(raw_df, "DATA_DATE"), errors="coerce")
    df["Region"] = _col(raw_df, "REGION_CODE", "Unknown").fillna("Unknown")
    df["Location"] = _col(raw_df, "LOCATION_CODE")
    df["Product"] = _col(raw_df, "PRODUCT_DESCRIPTION")

    system = _col(raw_df, "SOURCE_OPERATOR")
    if system.isna().all():
        system = _col(raw_df, "SOURCE_SYSTEM")
    df["System"] = system

    for out_col, raw_col in NUMERIC_COLUMN_MAP.items():
        df[out_col] = pd.to_numeric(_col(raw_df, raw_col, 0), errors="coerce").fillna(0)

    df["INVENTORY_KEY"] = _col(raw_df, "INVENTORY_KEY")
    df["SOURCE_FILE_ID"] = _col(raw_df, "SOURCE_FILE_ID")
    df["CREATED_AT"] = pd.to_datetime(_col(raw_df, "CREATED_AT"), errors="coerce")

    df["Notes"] = ""

    # Row lineage tracking (for SQLite we persist these columns; for Snowflake they may not exist)
    df["source"] = _col(raw_df, "source", "system").fillna("system")
    df["updated"] = pd.to_numeric(_col(raw_df, "updated", 0), errors="coerce").fillna(0).astype(int)

    # Midcon rows often use SOURCE_SYSTEM/OPERATOR differently; if System is missing, fall back to Location.
    midcon_mask = df["Region"] == "Midcon"
    needs_system = midcon_mask & df["System"].isna()
    df.loc[needs_system, "System"] = df.loc[needs_system, "Location"]

    return df


@st.cache_data(ttl=300, show_spinner=False)
def _load_inventory_data_cached(source: str, sqlite_db_path: str, sqlite_table: str) -> pd.DataFrame:
    if source == "sqlite":
        import sqlite3

        conn = sqlite3.connect(sqlite_db_path)
        raw_df = pd.read_sql_query(
            f"SELECT * FROM {sqlite_table} WHERE DATA_DATE IS NOT NULL ORDER BY DATA_DATE DESC, LOCATION_CODE",
            conn,
        )
        conn.close()
        return _normalize_inventory_df(raw_df)

    if source != "snowflake":
        raise ValueError("DATA_SOURCE must be 'snowflake' or 'sqlite'")

    session = get_snowflake_session()
    session.sql(f"USE WAREHOUSE {SNOWFLAKE_WAREHOUSE}").collect()

    query = f"""
    SELECT
        DATA_DATE,
        REGION_CODE,
        LOCATION_CODE,
        PRODUCT_DESCRIPTION,
        SOURCE_OPERATOR,
        CAST(COALESCE(RECEIPTS_BBL, 0) AS FLOAT) as RECEIPTS_BBL,
        CAST(COALESCE(DELIVERIES_BBL, 0) AS FLOAT) as DELIVERIES_BBL,
        CAST(COALESCE(RACK_LIFTINGS_BBL, 0) AS FLOAT) as RACK_LIFTINGS_BBL,
        CAST(COALESCE(CLOSING_INVENTORY_BBL, 0) AS FLOAT) as CLOSING_INVENTORY_BBL,
        CAST(COALESCE(OPENING_INVENTORY_BBL, 0) AS FLOAT) as OPENING_INVENTORY_BBL,
        CAST(COALESCE(PRODUCTION_BBL, 0) AS FLOAT) as PRODUCTION_BBL,
        CAST(COALESCE(PIPELINE_IN_BBL, 0) AS FLOAT) as PIPELINE_IN_BBL,
        CAST(COALESCE(PIPELINE_OUT_BBL, 0) AS FLOAT) as PIPELINE_OUT_BBL,
        CAST(COALESCE(TRY_TO_DOUBLE(ADJUSTMENTS_BBL), 0) AS FLOAT) as ADJUSTMENTS_BBL,
        CAST(COALESCE(TRY_TO_DOUBLE(GAIN_LOSS_BBL), 0) AS FLOAT) as GAIN_LOSS_BBL,
        CAST(COALESCE(TRY_TO_DOUBLE(TRANSFERS_BBL), 0) AS FLOAT) as TRANSFERS_BBL,
        CAST(COALESCE(TANK_CAPACITY_BBL, 0) AS FLOAT) as TANK_CAPACITY_BBL,
        CAST(COALESCE(SAFE_FILL_LIMIT_BBL, 0) AS FLOAT) as SAFE_FILL_LIMIT_BBL,
        CAST(COALESCE(AVAILABLE_SPACE_BBL, 0) AS FLOAT) as AVAILABLE_SPACE_BBL,
        INVENTORY_KEY,
        SOURCE_FILE_ID,
        CREATED_AT
    FROM {RAW_INVENTORY_TABLE}
    WHERE DATA_DATE IS NOT NULL
    ORDER BY DATA_DATE DESC, LOCATION_CODE, PRODUCT_CODE
    """

    raw_df = session.sql(query).to_pandas()
    return _normalize_inventory_df(raw_df)


def load_inventory_data() -> pd.DataFrame:
    return _load_inventory_data_cached(DATA_SOURCE, SQLITE_DB_PATH, SQLITE_TABLE)


def _normalize_source_status_df(raw_df: pd.DataFrame) -> pd.DataFrame:
    """Normalize source status records for UI display."""
    df = raw_df.copy()

    # Standardize column names to match CSV/table.
    # (SQLite table uses these exact names; Snowflake equivalent should match.)
    for c in ["RECEIVED_TIMESTAMP", "PROCESSED_AT"]:
        if c in df.columns:
            df[c] = pd.to_datetime(df[c], errors="coerce")

    # Prefer PROCESSED_AT when present, otherwise RECEIVED_TIMESTAMP
    if "PROCESSED_AT" in df.columns and "RECEIVED_TIMESTAMP" in df.columns:
        df["LAST_UPDATED_AT"] = df["PROCESSED_AT"].fillna(df["RECEIVED_TIMESTAMP"])
    elif "PROCESSED_AT" in df.columns:
        df["LAST_UPDATED_AT"] = df["PROCESSED_AT"]
    elif "RECEIVED_TIMESTAMP" in df.columns:
        df["LAST_UPDATED_AT"] = df["RECEIVED_TIMESTAMP"]
    else:
        df["LAST_UPDATED_AT"] = pd.NaT

    # Human friendly name for cards
    if "LOCATION" in df.columns:
        df["DISPLAY_NAME"] = df["LOCATION"].fillna("")
    else:
        df["DISPLAY_NAME"] = ""

    if "SOURCE_OPERATOR" in df.columns:
        op = df["SOURCE_OPERATOR"].fillna("")
    else:
        op = ""
    if "SOURCE_SYSTEM" in df.columns:
        sys = df["SOURCE_SYSTEM"].fillna("")
    else:
        sys = ""
    # Pick best available label
    df["SOURCE_LABEL"] = op
    if isinstance(sys, pd.Series):
        df.loc[df["SOURCE_LABEL"].astype(str).str.strip().eq(""), "SOURCE_LABEL"] = sys

    # Ensure REGION exists
    if "REGION" not in df.columns:
        df["REGION"] = "Unknown"
    df["REGION"] = df["REGION"].fillna("Unknown")

    # Standardize processing status
    if "PROCESSING_STATUS" in df.columns:
        df["PROCESSING_STATUS"] = df["PROCESSING_STATUS"].fillna("")
    else:
        df["PROCESSING_STATUS"] = ""

    return df


@st.cache_data(ttl=300, show_spinner=False)
def _load_source_status_cached(source: str, sqlite_db_path: str) -> pd.DataFrame:
    if source == "sqlite":
        import sqlite3

        conn = sqlite3.connect(sqlite_db_path)
        raw_df = pd.read_sql_query(
            f"SELECT * FROM {SQLITE_SOURCE_STATUS_TABLE}",
            conn,
        )
        conn.close()
        return _normalize_source_status_df(raw_df)

    if source != "snowflake":
        raise ValueError("DATA_SOURCE must be 'snowflake' or 'sqlite'")

    session = get_snowflake_session()
    session.sql(f"USE WAREHOUSE {SNOWFLAKE_WAREHOUSE}").collect()

    query = f"""
    SELECT
        CLASS,
        LOCATION,
        REGION,
        SOURCE_OPERATOR,
        SOURCE_SYSTEM,
        SOURCE_TYPE,
        FILE_ID,
        INTEGRATION_JOB_ID,
        FILE_NAME,
        SOURCE_PATH,
        PROCESSING_STATUS,
        ERROR_MESSAGE,
        WARNING_COLUMNS,
        RECORD_COUNT,
        RECEIVED_TIMESTAMP,
        PROCESSED_AT
    FROM {SNOWFLAKE_SOURCE_STATUS_TABLE}
    """

    raw_df = session.sql(query).to_pandas()
    return _normalize_source_status_df(raw_df)


def load_source_status() -> pd.DataFrame:
    return _load_source_status_cached(DATA_SOURCE, SQLITE_DB_PATH)


def initialize_data():
    """Initialize lightweight session state.

    We intentionally avoid loading the full inventory table up-front. Instead, we:
    - load available regions (small distinct query)
    - load source-status table (already small)

    Actual inventory data is loaded only when the user submits filters.
    """

    if "data_loaded" not in st.session_state:
        # Load and cache source freshness/status
        try:
            st.session_state.source_status = load_source_status()
        except Exception:
            # Don't block the app if status table isn't available
            st.session_state.source_status = pd.DataFrame()

        st.session_state.regions = load_regions()
        st.session_state.data_loaded = True

    return st.session_state.get("regions", [])


@st.cache_data(ttl=300, show_spinner=False)
def load_regions() -> list[str]:
    """Return all regions available in the source (distinct list)."""

    if DATA_SOURCE == "sqlite":
        import sqlite3

        conn = sqlite3.connect(SQLITE_DB_PATH)
        try:
            df = pd.read_sql_query(
                f"SELECT DISTINCT REGION_CODE AS Region FROM {SQLITE_TABLE} WHERE REGION_CODE IS NOT NULL ORDER BY REGION_CODE",
                conn,
            )
        finally:
            conn.close()
        return sorted(df["Region"].dropna().astype(str).unique().tolist())

    # Snowflake
    session = get_snowflake_session()
    session.sql(f"USE WAREHOUSE {SNOWFLAKE_WAREHOUSE}").collect()
    df = session.sql(
        f"SELECT DISTINCT REGION_CODE AS Region FROM {RAW_INVENTORY_TABLE} WHERE REGION_CODE IS NOT NULL ORDER BY REGION_CODE"
    ).to_pandas()
    return sorted(df["Region"].dropna().astype(str).unique().tolist())


@st.cache_data(ttl=300, show_spinner=False)
def load_region_filter_metadata(*, region: str | None, loc_col: str) -> dict:
    """Return lightweight metadata for sidebar filters: locations/systems + date bounds."""

    region_norm = _normalize_region_label(region) if region else None

    if DATA_SOURCE == "sqlite":
        import sqlite3

        conn = sqlite3.connect(SQLITE_DB_PATH)
        try:
            if loc_col == "System":
                sys_sql = f"""
                    SELECT DISTINCT
                        CASE
                            WHEN SOURCE_OPERATOR IS NOT NULL AND TRIM(SOURCE_OPERATOR) != '' THEN SOURCE_OPERATOR
                            WHEN SOURCE_SYSTEM IS NOT NULL AND TRIM(SOURCE_SYSTEM) != '' THEN SOURCE_SYSTEM
                            ELSE LOCATION_CODE
                        END AS System
                    FROM {SQLITE_TABLE}
                    WHERE DATA_DATE IS NOT NULL
                      AND (? IS NULL OR REGION_CODE = ?)
                    ORDER BY System
                """
                df_locs = pd.read_sql_query(sys_sql, conn, params=[region_norm, region_norm])
                locations = sorted(df_locs["System"].dropna().astype(str).unique().tolist())
            else:
                loc_sql = f"""
                    SELECT DISTINCT LOCATION_CODE AS Location
                    FROM {SQLITE_TABLE}
                    WHERE DATA_DATE IS NOT NULL
                      AND (? IS NULL OR REGION_CODE = ?)
                      AND LOCATION_CODE IS NOT NULL
                    ORDER BY Location
                """
                df_locs = pd.read_sql_query(loc_sql, conn, params=[region_norm, region_norm])
                locations = sorted(df_locs["Location"].dropna().astype(str).unique().tolist())

            dates_sql = f"""
                SELECT MIN(DATA_DATE) AS min_date, MAX(DATA_DATE) AS max_date
                FROM {SQLITE_TABLE}
                WHERE DATA_DATE IS NOT NULL
                  AND (? IS NULL OR REGION_CODE = ?)
            """
            df_dates = pd.read_sql_query(dates_sql, conn, params=[region_norm, region_norm])
        finally:
            conn.close()

        min_date = pd.to_datetime(df_dates.iloc[0]["min_date"], errors="coerce") if not df_dates.empty else pd.NaT
        max_date = pd.to_datetime(df_dates.iloc[0]["max_date"], errors="coerce") if not df_dates.empty else pd.NaT
        return {
            "locations": locations,
            "min_date": min_date,
            "max_date": max_date,
        }

    # Snowflake
    session = get_snowflake_session()
    session.sql(f"USE WAREHOUSE {SNOWFLAKE_WAREHOUSE}").collect()
    region_escaped = str(region_norm).replace("'", "''") if region_norm else ""
    region_filter = "" if not region_norm else f" AND REGION_CODE = '{region_escaped}'"

    if loc_col == "System":
        loc_query = f"""
            SELECT DISTINCT
                COALESCE(NULLIF(SOURCE_OPERATOR, ''), NULLIF(SOURCE_SYSTEM, ''), LOCATION_CODE) AS System
            FROM {RAW_INVENTORY_TABLE}
            WHERE DATA_DATE IS NOT NULL {region_filter}
            ORDER BY System
        """
        df_locs = session.sql(loc_query).to_pandas()
        locations = sorted(df_locs["SYSTEM"].dropna().astype(str).unique().tolist()) if "SYSTEM" in df_locs.columns else []
    else:
        loc_query = f"""
            SELECT DISTINCT LOCATION_CODE AS Location
            FROM {RAW_INVENTORY_TABLE}
            WHERE DATA_DATE IS NOT NULL {region_filter}
              AND LOCATION_CODE IS NOT NULL
            ORDER BY Location
        """
        df_locs = session.sql(loc_query).to_pandas()
        locations = sorted(df_locs["LOCATION"].dropna().astype(str).unique().tolist()) if "LOCATION" in df_locs.columns else []

    date_query = f"""
        SELECT MIN(DATA_DATE) AS min_date, MAX(DATA_DATE) AS max_date
        FROM {RAW_INVENTORY_TABLE}
        WHERE DATA_DATE IS NOT NULL {region_filter}
    """
    df_dates = session.sql(date_query).to_pandas()
    min_date = pd.to_datetime(df_dates.iloc[0]["MIN_DATE"], errors="coerce") if not df_dates.empty else pd.NaT
    max_date = pd.to_datetime(df_dates.iloc[0]["MAX_DATE"], errors="coerce") if not df_dates.empty else pd.NaT
    return {"locations": locations, "min_date": min_date, "max_date": max_date}


def ensure_numeric_columns(df_filtered: pd.DataFrame) -> pd.DataFrame:
    for col in NUMERIC_COLUMN_MAP.keys():
        if col in df_filtered.columns:
            df_filtered[col] = pd.to_numeric(df_filtered[col], errors="coerce").fillna(0)
    return df_filtered


def _normalize_region_label(active_region: str | None) -> str | None:
    """Normalize UI region labels to match data Region values."""
    if active_region is None:
        return None
    return "Midcon" if active_region == "Group Supply Report (Midcon)" else active_region


def create_sidebar_filters(regions: list[str], df_region: pd.DataFrame) -> dict:
    """Create sidebar filters UI.

    This replaces sidebar_filters.py.

    Key behavior:
    - Location/System is **single-select**
    - Product filter removed
    - Designed to be used with a submit button (data loads on submit)

    IMPORTANT UX NOTE:
    The Region selector is expected to live **outside** the form so that changing
    Region immediately triggers a rerun and we can refresh the Location/System
    options without requiring the user to hit Submit.
    """

    active_region = st.session_state.get("active_region")

    # Location/System selector (options depend on active_region)
    loc_col = "System" if _normalize_region_label(active_region) == "Midcon" else "Location"
    filter_label = "ðŸ­ System" if loc_col == "System" else "ðŸ“ Location"

    if df_region is not None and not df_region.empty and loc_col in df_region.columns:
        locations = sorted(df_region[loc_col].dropna().unique().tolist())
        df_min = df_region["Date"].min() if "Date" in df_region.columns else pd.NaT
        df_max = df_region["Date"].max() if "Date" in df_region.columns else pd.NaT
    else:
        meta = load_region_filter_metadata(region=active_region, loc_col=loc_col)
        locations = meta.get("locations", [])
        df_min = meta.get("min_date", pd.NaT)
        df_max = meta.get("max_date", pd.NaT)

    # If user previously selected a location that isn't in this region anymore,
    # reset it so Streamlit doesn't get stuck with an invalid widget state.
    prev_loc = st.session_state.get("selected_loc")
    if prev_loc is not None and prev_loc not in locations:
        st.session_state.selected_loc = None

    if not locations:
        st.warning("No locations available")
        selected_loc = None
    else:
        # Streamlit selectbox requires an integer index.
        # If we already have a valid selection, point the widget at it.
        current = st.session_state.get("selected_loc")
        index = locations.index(current) if current in locations else 0
        selected_loc = st.selectbox(filter_label, options=locations, index=index, key="selected_loc")

    # Date range selector
    # NOTE ON CIRCULAR IMPORTS:
    # We import `get_default_date_window` *inside* this function to avoid a
    # module-level circular import:
    #   admin_config -> imports data_loader
    #   data_loader  -> importing admin_config at import-time would recurse
    # This is not an infinite loop at runtime; it's simply a safe import pattern.
    today = date.today()
    scope_location = None if selected_loc is None else str(selected_loc)
    from admin_config import get_default_date_window

    start_off, end_off = get_default_date_window(
        region=_normalize_region_label(active_region or "Unknown") or "Unknown",
        location=scope_location,
    )
    default_start = today + timedelta(days=int(start_off))
    default_end = today + timedelta(days=int(end_off))

    df_min_d = pd.to_datetime(df_min, errors="coerce").date() if pd.notna(df_min) else default_start
    df_max_d = pd.to_datetime(df_max, errors="coerce").date() if pd.notna(df_max) else default_end

    min_value = min(df_min_d, default_start)
    max_value = max(df_max_d, default_end)

    # Default: show DB min if older than configured start; always extend to max available.
    actual_start = df_min_d if df_min_d < default_start else default_start
    actual_end = max_value

    date_range = st.date_input(
        "Date Range",
        value=(actual_start, actual_end),
        min_value=min_value,
        max_value=max_value,
        key=f"date_{active_region}_{scope_location or 'all'}",
    )

    if isinstance(date_range, (list, tuple)):
        if len(date_range) == 2:
            start_date, end_date = date_range
        else:
            start_date = end_date = date_range[0] if date_range else date.today()
    else:
        start_date = end_date = date_range

    start_ts, end_ts = pd.to_datetime(start_date), pd.to_datetime(end_date)

    return {
        "active_region": active_region,
        "start_ts": start_ts,
        "end_ts": end_ts,
        "selected_loc": selected_loc,
        "loc_col": loc_col,
        "locations": locations,
    }


def apply_filters(df_region: pd.DataFrame, filters: dict) -> pd.DataFrame:
    """Apply the selected filters to the dataframe (in-memory).

    Note: with the new submit-driven querying, this will often be applied to
    already-filtered DB results. It's still useful as a safety net.
    """

    df_filtered = df_region.copy()

    if df_filtered.empty:
        return df_filtered

    # Apply date filter
    if "Date" in df_filtered.columns:
        df_filtered = df_filtered[
            (df_filtered["Date"] >= filters["start_ts"]) &
            (df_filtered["Date"] <= filters["end_ts"])
        ]

    # Apply location/system filter
    loc_col = filters.get("loc_col", "Location")
    selected_loc = filters.get("selected_loc")
    if selected_loc and loc_col in df_filtered.columns:
        df_filtered = df_filtered[df_filtered[loc_col].isin([selected_loc])]

    return df_filtered


@st.cache_data(ttl=300, show_spinner=False)
def _load_inventory_data_filtered_cached(
    source: str,
    sqlite_db_path: str,
    sqlite_table: str,
    *,
    region: str | None,
    loc_col: str,
    selected_loc: str | None,
    start_ts: pd.Timestamp,
    end_ts: pd.Timestamp,
) -> pd.DataFrame:
    """Load inventory data filtered at the source (SQLite/Snowflake).

    This is the performance-critical path: we avoid loading the entire dataset
    when users only need a small slice.
    """

    region_norm = _normalize_region_label(region) if region else None

    if source == "sqlite":
        import sqlite3

        start_s = pd.Timestamp(start_ts).strftime("%Y-%m-%d")
        end_s = pd.Timestamp(end_ts).strftime("%Y-%m-%d")

        where = ["DATA_DATE IS NOT NULL", "DATA_DATE >= ?", "DATA_DATE <= ?"]
        params: list[object] = [start_s, end_s]

        if region_norm:
            where.append("REGION_CODE = ?")
            params.append(region_norm)

        # We filter by LOCATION_CODE in SQL for both Location and System.
        # For Midcon/System, System is normalized from SOURCE_OPERATOR/SOURCE_SYSTEM,
        # which isn't reliably filterable in SQLite without complex logic.
        # If the user selects a System, we apply it after normalization.
        if selected_loc and loc_col == "Location":
            where.append("LOCATION_CODE = ?")
            params.append(str(selected_loc))

        sql = f"""
            SELECT *
            FROM {sqlite_table}
            WHERE {' AND '.join(where)}
            ORDER BY DATA_DATE DESC, LOCATION_CODE
        """

        conn = sqlite3.connect(sqlite_db_path)
        raw_df = pd.read_sql_query(sql, conn, params=params)
        conn.close()

        df = _normalize_inventory_df(raw_df)
        if selected_loc and loc_col == "System":
            df = df[df["System"].astype(str) == str(selected_loc)]
        return df

    if source != "snowflake":
        raise ValueError("DATA_SOURCE must be 'snowflake' or 'sqlite'")

    session = get_snowflake_session()
    session.sql(f"USE WAREHOUSE {SNOWFLAKE_WAREHOUSE}").collect()

    # Snowflake filter pushdown
    conditions = ["DATA_DATE IS NOT NULL", "DATA_DATE >= %(start)s", "DATA_DATE <= %(end)s"]
    binds: dict[str, object] = {
        "start": pd.Timestamp(start_ts).strftime("%Y-%m-%d"),
        "end": pd.Timestamp(end_ts).strftime("%Y-%m-%d"),
    }
    if region_norm:
        conditions.append("REGION_CODE = %(region)s")
        binds["region"] = region_norm
    if selected_loc and loc_col == "Location":
        conditions.append("LOCATION_CODE = %(loc)s")
        binds["loc"] = str(selected_loc)

    where_sql = " AND ".join(conditions)
    query = f"""
    SELECT
        DATA_DATE,
        REGION_CODE,
        LOCATION_CODE,
        PRODUCT_DESCRIPTION,
        SOURCE_OPERATOR,
        CAST(COALESCE(RECEIPTS_BBL, 0) AS FLOAT) as RECEIPTS_BBL,
        CAST(COALESCE(DELIVERIES_BBL, 0) AS FLOAT) as DELIVERIES_BBL,
        CAST(COALESCE(RACK_LIFTINGS_BBL, 0) AS FLOAT) as RACK_LIFTINGS_BBL,
        CAST(COALESCE(CLOSING_INVENTORY_BBL, 0) AS FLOAT) as CLOSING_INVENTORY_BBL,
        CAST(COALESCE(OPENING_INVENTORY_BBL, 0) AS FLOAT) as OPENING_INVENTORY_BBL,
        CAST(COALESCE(PRODUCTION_BBL, 0) AS FLOAT) as PRODUCTION_BBL,
        CAST(COALESCE(PIPELINE_IN_BBL, 0) AS FLOAT) as PIPELINE_IN_BBL,
        CAST(COALESCE(PIPELINE_OUT_BBL, 0) AS FLOAT) as PIPELINE_OUT_BBL,
        CAST(COALESCE(TRY_TO_DOUBLE(ADJUSTMENTS_BBL), 0) AS FLOAT) as ADJUSTMENTS_BBL,
        CAST(COALESCE(TRY_TO_DOUBLE(GAIN_LOSS_BBL), 0) AS FLOAT) as GAIN_LOSS_BBL,
        CAST(COALESCE(TRY_TO_DOUBLE(TRANSFERS_BBL), 0) AS FLOAT) as TRANSFERS_BBL,
        CAST(COALESCE(TANK_CAPACITY_BBL, 0) AS FLOAT) as TANK_CAPACITY_BBL,
        CAST(COALESCE(SAFE_FILL_LIMIT_BBL, 0) AS FLOAT) as SAFE_FILL_LIMIT_BBL,
        CAST(COALESCE(AVAILABLE_SPACE_BBL, 0) AS FLOAT) as AVAILABLE_SPACE_BBL,
        INVENTORY_KEY,
        SOURCE_FILE_ID,
        CREATED_AT
    FROM {RAW_INVENTORY_TABLE}
    WHERE {where_sql}
    ORDER BY DATA_DATE DESC, LOCATION_CODE, PRODUCT_CODE
    """

    # Bind substitution (safe basic string quoting)
    for k, v in binds.items():
        query = query.replace(f"%({k})s", "'" + str(v).replace("'", "''") + "'")

    raw_df = session.sql(query).to_pandas()
    df = _normalize_inventory_df(raw_df)
    if selected_loc and loc_col == "System":
        df = df[df["System"].astype(str) == str(selected_loc)]
    return df


def load_filtered_inventory_data(filters: dict) -> pd.DataFrame:
    """Load inventory data using filter pushdown."""
    return _load_inventory_data_filtered_cached(
        DATA_SOURCE,
        SQLITE_DB_PATH,
        SQLITE_TABLE,
        region=filters.get("active_region"),
        loc_col=str(filters.get("loc_col") or "Location"),
        selected_loc=(None if filters.get("selected_loc") in (None, "") else str(filters.get("selected_loc"))),
        start_ts=pd.Timestamp(filters.get("start_ts")),
        end_ts=pd.Timestamp(filters.get("end_ts")),
    )


def require_selected_location(filters: dict) -> None:
    """Enforce that a location/system must be selected before loading data."""
    # NOTE:
    # `st.stop()` does not cause recursion; it just halts the current Streamlit
    # script run. Streamlit will re-run normally on the next user interaction.
    if filters.get("selected_loc") in (None, ""):
        st.warning("Please select a Location/System before submitting filters.")
        st.stop()


@st.cache_data(ttl=300, show_spinner=False)
def load_region_location_pairs() -> pd.DataFrame:
    """Small helper for admin UI: distinct Region/Location pairs."""

    if DATA_SOURCE == "sqlite":
        import sqlite3

        conn = sqlite3.connect(SQLITE_DB_PATH)
        try:
            df = pd.read_sql_query(
                f"""
                SELECT DISTINCT
                    REGION_CODE AS Region,
                    LOCATION_CODE AS Location
                FROM {SQLITE_TABLE}
                WHERE REGION_CODE IS NOT NULL
                  AND LOCATION_CODE IS NOT NULL
                """,
                conn,
            )
        finally:
            conn.close()
        return df

    session = get_snowflake_session()
    session.sql(f"USE WAREHOUSE {SNOWFLAKE_WAREHOUSE}").collect()
    query = f"""
        SELECT DISTINCT
            REGION_CODE AS Region,
            LOCATION_CODE AS Location
        FROM {RAW_INVENTORY_TABLE}
        WHERE REGION_CODE IS NOT NULL
          AND LOCATION_CODE IS NOT NULL
    """
    return session.sql(query).to_pandas()
